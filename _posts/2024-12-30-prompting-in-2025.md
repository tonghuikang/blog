---
layout: post
title: Competitive Programming Supercommunity
categories: ai prompt-engineering
---
I write my wishlist on prompt engineering in 2025.


## My current prompting workflow in 2024

If I am tasked to write a classification prompt, it takes me six full engineering days at minimum.

- Two full days is needed to understand the problem and source for real input examples.
	- I need to think what is the average user experience (the impression sample)
	- I need to think what is the average negative case (the negative action sample)
	- I need to think of edge cases - cases where failure will cause an outsized impact (for example users that query whether how many rocks should you eat in a day)
	- The product owner may give me some examples to start with, but I still need to find all these sets of examples.
	- In this period, I will not be writing any prompts.
- Four full days is needed to search for a prompt that works.
	- I will start with the cheapest model available (currently Gemini-1.5-Flash if I am not concerned with their uptime), and choose a more expensive model if necessary.
	- I will also start with the simplest prompt possible.
	- Prompt improvements is usually done by adding few-shot examples where the input has issues. Whenever I add examples from the dataset into the prompt, I need to add more examples into my dataset so that I am not just testing on the training set.
	- From understanding the outputs the model gives, I could think of more edge cases where the model could fail, and I need to find and add them.
	- Sometimes when the classification from the model disagrees with what I labelled in my dataset. I will update the dataset.
	- Sometimes I find that there are examples where it is very borderline and I am okay with either classification. I will remove the example from the evaluation of the performance.
- Additional time is needed to analyze on the product impact
	- How would the user experience change because of my prompts?
	- What is the expected rate of regrettable false positive and regrettable false negatives?
	- Are we happy enough with the performance to ship it, we should I continue to attempt to fix the regrettable mistakes?
	- I commit these analysis scripts to the monorepo.
- Additional time is needed if I need to realign 
	- It is not always the case the product owner or leadership agrees with my prompt and dataset.
	- Sometimes they find some sample in the dataset where the intended classification should be positive instead of negative.
	- Sometimes they find some sample in the dataset must be classified as positive rather than being okay with either classification.
	- To avoid changing the prompt that I have a lot of time on, I give constant updates on my progress with the dataset and the prompt.


If I am tasked to write a generation prompt, it also takes me six full engineering days at minimum.

- Two full days is needed to understand the problem and source for real input examples.
	- I need to think what is the average user experience (the impression sample)
	- I need to think what is the average negative case (the negative action sample)
	- I need to think of edge cases - cases where failure will cause an outsized impact (for example users that query whether how many rocks should you eat in a day)
	- The product owner may give me some examples to start with, but I still need to find all these sets of examples.
	- In this period, I will not be writing any prompts.
- Four full days is needed to search for a prompt that works.
	- I will start with the cheapest model available (currently Gemini-1.5-Flash if I am not concerned with their uptime), and choose a more expensive model if necessary.
	- I will also start with the simplest prompt possible.
	- Prompt improvements is usually done by adding few-shot examples where the input has issues. Whenever I add examples from the dataset into the prompt, I need to add more examples into my dataset so that I am not just testing on the training set.
	- From understanding the outputs the model gives, I could think of more edge cases where the model could produce bad generation, and I need to find and add them.
- Additional time is needed to analyze on the product impact
	- How would the user experience change because of my prompts?
	- What is the expected rate of regrettable bad generations?
	- Are we happy enough with the performance to ship it, we should I continue to attempt to fix the regrettable mistakes?
	- I commit these analysis scripts to the monorepo.
- Additional time is needed if I need to realign
	- It is not always the case the product owner or leadership agrees with my prompt and dataset.
	- Sometimes they found some new inputs that my prompt must get correct.
	- Sometimes they disagree with the expected output in my few shot examples.
	- To avoid changing the prompt that I have a lot of time on, I give constant updates on my progress with the dataset and the prompt.
- I usually need to write classification prompts along with generation prompts
	- Classification prompts could be used to evaluate my generation prompt (more commonly known as LLM as a judge). Maybe we can ship only if the generation prompt achieves a certain performance as measured by the classification prompt.
	- Classification prompts could be used to decide whether should you even do generation (for example, do not advice the user on whether how many rocks should a human eat)
	- Classification prompts could be used to decide whether an output is better than the other. (However, in practice, writing the classifier could be much [harder](https://nbviewer.org/github/tonghuikang/judge-to-generation-prompt/blob/master/notebook.ipynb) than writing the generation prompt).
	- A similar number of days is needed for me to align each classification prompt - but I save quite a lot of time hunting for input samples.


Alignment is important.

The best prompting trick I have learnt is to print table to html. It is easy to export html text to Google sheets.

```python
output_table_file_name = "output.html"
with open(output_table_file_name, 'w') as f:
    f.write('<meta charset="UTF-8">\n' + df.replace({r'\n': '<br>'}, regex=True).to_html(index=False, escape=False))

from IPython.display import display, HTML

filepath = os.getcwd().replace("/efs/notebooks", "https://redacted.company.net/") + "/" + output_table_file_name
link = f'<a href="{filepath}" target="_blank">{filepath}</a>'
display(HTML(link))
```

The second best prompting trick I learnt is to download from Google sheets (requires the Google sheets to be accessible by anyone with the URL).

```python
%%bash
curl -L -o llm_topic_eval_set.csv \
"https://docs.google.com/spreadsheets/d/redacted_RandomSetOfAlphaNumericCharactersWithUnderscores/export?exportFormat=csv"
```

This summarizes my prompting workflow in 2024.



## Model providers should automate prompting

In 2025, I hope to pass the model providers my dataset and they give me a prompt that I can use for their model. We should start with classification.

Currently, few shot prompting achieves this effect. But there is no guarantee that my few shot prompt is the best among all possible prompts. I need to communicate with confidence and conscience, with my engineering reputation at stake, to my product owner that this is the best prompt I have tried.

Since my company has spent enough money with Anthropic, I can ask one of their friendly solution engineers to come up with a better prompt for me. This way less of my engineering reputation is at stake. Another strategy is to get the product owner to try prompting to see if they can produce a better prompt. Ultimately, my job as a prompt engineer is not to write prompts, but to measure how bad are the prompts.

Hopefully Anthropic can provide a self-serve solution so I do not need to bother their solution engineers. Anthropic should also optimize their models to that they perform well on prompting tasks.

I think OpenAI already has plans to provide an automated prompting solution. Their [Evals](https://platform.openai.com/docs/guides/evals) tool seeks [permission](https://help.openai.com/en/articles/10306912-sharing-feedback-evals-and-api-data-with-openai) from the developer to share their prompts.




## The aligned prompt optimizer should be provider-agnostic

While Anthropic solution engineers are friendly, my interests do not exactly align with Anthropic engineers.

If I want to switch from using OpenAI models to Anthropic models for a particular use case, Anthropic engineers will be very interested to help me. However, if I want to switch from Claude-3.5-Sonnet to Claude-3.5-Haiku for cost reasons, our interests might not align if Anthropic is interested in total revenue.

The prompt optimizer should be aligned to me. Instead of spending an hour of my salaried time on a prompt iteration, I think the company is happy to pay a prompt optimizer ten dollars for every prompt iteration. I do not need to put my engineering reputation at stake. The prompt optimizer could be incentivized by only charging if they manage to find a prompt that is much better than mine.

When the job is classification, the job is classification. It should not matter which model provider we use, or what techniques we employ. We should only care about the cost-performance tradeoff. The prompt optimizer that aligns with my interest should be provider-agnostic.

Solutions on the cost-performance frontier may not be just prompt engineering. Maybe with enough data, we can [distill](https://platform.openai.com/docs/guides/distillation) a closed sourced model to another closed sourced model. Maybe the cheapest solution involves a fine-tuned BERT model if you have millions of labels. Maybe eventually the solution with the best tradeoff is still generic models served at scale.

There is still a role for model providers. Model providers are incentivized to work with the provider-agnostic prompt optimizers to make sure that their prompts to their models are correctly optimized.




## Benchmarks should accelerate the progress

There are two benchmark which I think could be great Kaggle competitions

- Given multiple (maybe 50 public, 50 private) datasets of (input -> label, split into public and private) and an LLM, write a program to find the text prompt that best fits the datasets. This should evaluate the ability of different automated prompt engineering techniques.
- Given multiple (maybe 50 public, 50 private) datasets of (input -> label, split into public and private), write a program that produce a function that best fits the datasets. This should evaluate the ability of text classification techniques, which may or may not involve prompt engineering.

I have [tried](https://github.com/tonghuikang/automatic-prompt-engineer) to build an automated prompt engineer on one of my datasets, and the performance is worse than random.

In 2025, I hope to do less prompt engineering, and spend the time on aligning the dataset.
