---
layout: post
title: Tasks that are easy to describe and grade
---

I list some tasks that should that are easy to describe and easy to grade - that are not yet fully solved by AI. I predict and expect huge progress to be made soon, if not already.

Even though the tasks are easy to grade and easy to describe, it doesn't mean that the task is easy to create.



These are some skills I expect to do well.

This does not mean that they are economically useful.

But the capability to train one helps with the capability to train the others.

There are some interesting ways to train them.

The purpose of a school is to teach how to learn

The purpose of training these is to show you can get models to learn advanced tasks.

Although many of these tasks is not economically valuable on its own

The purpose of a math college is not just to produce people educated in math, but people who are about to think hard about problems.



# Competitive programming

I expect competitive programming to be the "hello world" for AGI.

Problems are also easy to invent.

We should be able to expect models to do well in all competitive programming problems.

I have described this in some detail in another blogpost.

Models are already good, and we want to scale this up.




# Mathematical Integration

MIT integration challenge

Would be surprised if AI hasn't solve this.

Other mathematical operations that are verifiable - multiplication (with reasoning)

Other branches of mathematics is not easily verifiable.


https://math.mit.edu/~yyao1/integrationbee.html

It is easier to find something interesting to differentiate.

If a complex integral yields a simple result, it makes sense .

You can have code to verify the correctness of integration.

There are symbolic ways to check correctness - e.g. Taylor expansion

What mathematical tasks are there though




# Drawing ASCII text

LLMs are bad at drawing ASCII

LLMs are already generally good at recognizing images, especially those without detail.

Game of telephone? Similar to text translation

I would expect image generation to be trained this way too.




# Prompt engineering to a well-defined objective

Prompt engineering to do well in a classifier for example.

I had this example

Include the table of questions etc

The AI is given access to a model

100 classification results without cheating

Okay to include as few shot, for evaluation, for testing.

This is actually economically useful. I wonder how much of input and output tokens are fixed, weighted by price. Probably 50%?

This does not question the objective.

Overfitting - maybe - depends on test set.

This requires collecting real world test cases though. Need to find meaningful classification to do.

Pattern finding?






# Heuristic competitions

There is gradient that you can climb

Google Hash Code

There are various ways to etc

AtCoder Heuristic competition

As long as you can implement the judge

Debugging statements

Learning how to analyze your code, learning how to debug your output are emergent abilities we want to train the LLMs for.




# Battle bots

Kaggle

Examples of such compeititon on Kaggle

Examples of competition elsewhere

The difference with heuristic competitions is that your opponent is dynamic now.

Learning how to analyze your code, learning how to debug your output are emergent abilities we want to train the LLMs for.


