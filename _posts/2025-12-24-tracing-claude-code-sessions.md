---
layout: post
title: Your coding agent should be multichannel
---

I have build yet another visualizer for Claude Code sessions

https://tonghuikang.github.io/claude-code-sessions/?project=claude-code-tracing

This is a feature walkthrough
- (please fill up)
- Claude Code may invoke subagents. You can see the interactions with the subagent. (citation)


I may be updating more sessions, but I expect this process to be slow. The main objective of making this prototype is to write this blogpost, which I have achieved.



Reasons that I am building this

- Visualize my own Claude Code sessions
- Share how I prompt the model, how I would like to not prompt the model
- Deliver some views on evaluation and training could be done



## How I prompt the AI coding tool

I provide the context in bits and pieces. Usually I do not write the fully query. I expect to both get helpful results and steer the conversation in real time. This should soon be a baseline expectation for any chat product.
- (todo: add examples)


Please confirm that it is an issue, and fix the issue
- (find an example)
- Notice that I prompt the model
- If the model is better, I would have prompt - fix this issue. They could also do the reproduction and code research in parallel and share findings with each other.


Please show me a working examples
- (something about the display the url)

Did you check?
- If the model claims that something could not be done, they should provide a citation. Ideally the citation should come from the Internet, or a string from the message
	- jsonl files





## Shortcomings of the model

Do not declare completion when there is something obviously needs to be fixed

Check their own work. For example I ask for the number of impressions a piece of content have. I do not just want only the answer. I want the proof. But you should still make the answer obvious for me.

Make their own work easily checkable



## How evaluation and training could be done


Evaluation

- I want all my interactions with AI to be recorded, so I can annotate and share to promote knowledge on how to interact with AI
- Ideally this is something that Claude Code should have build


What I expect frontier companies to be collecting

- Every follow-up message either contains new information, or AI is doing.
- I would expect finetuning to happen on the action level. You cannot reproduce the environment, because you do not have access to that.
- There should be a evaluator. Similar to chess, you make a move, the Stockfish evaluator bar goes up or down. 



## What could AI coding tools provide

Ideally I want visualize all my Claude Code sessions on the web and share them. (What other AI coding tool have this feature?)


This only shows the process of building the visualizer itself. I have some projects that I have no yet published. I hope to add more, but it will be slow.


Currently the default for AI coding tools. Claude Code gets you to opt in although the default .
Other AI coding tools do not show you an opt-in screen. It is only the enterprise offering.


Ideally AI coding tool provider should identify what should be learnt, at the AI coding tool's cost. You just need the information on whether the scenario could fail, you do not need the whole trajectory data (you can see that I am taking a risk myself by sharing my session data). I understand AI coding tools. Interactions with AI is actually more valuable than published information out there, interactions with your AI is even more valuable. I understand that to gain an edge over the competitors you will need to.




