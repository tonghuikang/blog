---
layout: post
title: Reading papers
---
I am moving from one place to another. I printed a plenty of papers and I want to clear them out.



# Large Language Model papers




# Recommendation Systems papers



Two tower model

- Self-supervised Learning for Large-scale Item Recommendations [[link](https://arxiv.org/abs/2007.12865)] Google, 25 Jul 2020
- Cross-Batch Negative Sampling for Training Two-Tower Recommenders [[link](https://arxiv.org/abs/2110.15154)] Huawei, 28 Oct 2021
- Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations [[link](https://dl.acm.org/doi/10.1145/3366424.3386195)] Google, 20 Apr 2020
- Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations [[link](https://dl.acm.org/doi/abs/10.1145/3298689.3346996)] Google, 10 Sep 2019
- Deep Neural Networks for YouTube Recommendations [[link](https://dl.acm.org/doi/10.1145/2959100.2959190)] Google, 15 Sep 2016


User interest exploration

- Values of User Exploration in Recommender Systems [[link](https://dl.acm.org/doi/10.1145/3460231.3474236)] Google, 13 Sep 2021


Item exploration

- Nonlinear Bandits Exploration for Recommendations [[link](https://dl.acm.org/doi/10.1145/3604915.3610245)] Google, 14 Sep 2023
- Online Matching: A Real-time Bandit System for Large-scale Recommendations [[link](https://arxiv.org/abs/2307.15893)] Google, 29 Jul 2023
- Long-Term Value of Exploration: Measurements, Findings and Algorithms [[link](https://arxiv.org/abs/2305.07764)] Google, 12 May 2023
- Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation [[link](https://arxiv.org/abs/2306.01720)] Google, 2 Jun 2023


Trainable embeddings

- Monolith: Real Time Recommendation System With Collisionless Embedding Table [[link](https://arxiv.org/abs/2209.07663)] ByteDance, 16 Sep 2022
- Efficient Data Representation Learning in Google-scale Systems [[link](https://dl.acm.org/doi/10.1145/3604915.3608882)] Google, 14 Sep 2023



Pretrained embeddings

- Cross-lingual Language Model Pretraining [[link](https://arxiv.org/abs/1901.07291)] Facebook AI, 22 Jan 2019
- Text Embeddings by Weakly-Supervised Contrastive Pre-training [[link](https://arxiv.org/abs/2212.03533)] Microsoft, 7 Dec 2022



Sequence feature modelling

- TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest [[link](https://arxiv.org/abs/2306.00248)] Pinterest, 1 Jun 2023
- Behavior Sequence Transformer for E-commerce Recommendation in Alibaba [[link](https://arxiv.org/abs/1905.06874)] Alibaba, 16 May 2019
- Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction [[link](https://arxiv.org/abs/2006.05639)] Alibaba, 10 Jun 2020
- DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems [[link](https://arxiv.org/abs/2008.13535)] Google, 31 Aug 2020
- Deep Interest Network for Click-Through Rate Prediction [[link](https://arxiv.org/abs/1706.06978)] Alibaba, 21 Jun 2017



Calibration

- On Calibration of Modern Neural Networks [[link](https://arxiv.org/abs/1706.04599)] Cornell University, 14 Jun 2017
- Why Model Calibration Matters and How to Achieve It [[link](https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html)] Google, Apr 2021
- Multi-task Learning and Calibration for Utility-based Home Feed Ranking [[link](https://medium.com/pinterest-engineering/multi-task-learning-and-calibration-for-utility-based-home-feed-ranking-64087a7bcbad)] Pinterest, 14 Sep 2020
- The Foundations of Cost-Sensitive Learning [[link](https://dl.acm.org/doi/10.5555/1642194.1642224)] UCSD, 4 Aug 2001
- Predicting Good Probabilities with Supervised Learning [[link](https://dl.acm.org/doi/10.1145/1102351.1102430)] Cornell, 7 Aug 2005



Training multi-task models

- Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts [[link](https://dl.acm.org/doi/10.1145/3219819.3220007)] Google, 13 Jun 2018
- Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations [[link](https://dl.acm.org/doi/10.1145/3383313.3412236)] Tencent, 22 Sep 2020
- Recommending What Video to Watch Next: A Multitask Ranking System [[link](https://dl.acm.org/doi/10.1145/3298689.3346997)] Google, 10 Sep 2019



Value modelling

- What We Know About Using Non-Engagement Signals in Content Ranking [[link](https://arxiv.org/abs/2402.06831)] Integrity Institute, 9 Feb 2024
- Multi-Objective Recommendation via Multivariate Policy Learning [[link](https://arxiv.org/abs/2405.02141)] Spotify, 3 May 2024
- Feedback Shaping: A Modeling Approach to Nurture Content Creation [[link](https://arxiv.org/abs/2106.11312)] LinkedIn, 21 Jun 2021



Recommendation as sequence prediction

- Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations [[link](https://arxiv.org/abs/2402.17152)] Meta, 26 Feb 2024
- Effective and Efficient Training for Sequential Recommendation using Recency Sampling [[link](https://arxiv.org/abs/2207.02643)] University of Glasgow, 6 Jul 2022
- BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer [[link](https://arxiv.org/abs/1904.06690)] Alibaba, 15 Apr 2019
- Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders [[link](https://arxiv.org/abs/2308.12256)] Google, 23 Aug 2023



Deep retrieval

- Deep Retrieval: Learning A Retrievable Structure for Large-Scale Recommendations [[link](https://arxiv.org/abs/2007.07203)] ByteDance, 14 Jul 2020



Miscellaneous

- Improving Training Stability for Multitask Ranking Models in Recommender Systems [[link](https://arxiv.org/abs/2302.09178)] Google, 18 Feb 2023
- Trustworthy Online Marketplace Experimentation with Budget-split Design [[link](https://arxiv.org/abs/2012.08724)] LinkedIn, 16 Dec 2020
- Why do tree-based models still outperform deep learning on tabular data? [[link](https://arxiv.org/abs/2207.08815)] 18 Jul 2022
- Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models [[link](https://arxiv.org/abs/2209.06053)] Alibaba, 13 Sep 2022
- Fairness in Recommendation Ranking through Pairwise Comparisons [[link](https://arxiv.org/abs/1903.00780)] Google, 2 Mar 2019
- Practical Lessons from Predicting Clicks on Ads at Facebook [[link](https://dl.acm.org/doi/10.1145/2648584.2648589)] Facebook, 24 Aug 2014
- Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [[link](https://arxiv.org/abs/1603.09320)] Russian Academy of Sciences, 30 Mar 2016
- Deep Retrieval: Learning A Retrievable Structure for Large-Scale Recommendations [[link](https://arxiv.org/abs/2007.07203)] ByteDance, 14 Jul 2020
- Full Index Deep Retrieval: End-to-End User and Item Structures for Cold-start and Long-tail Item Recommendation [[link](https://dl.acm.org/doi/10.1145/3604915.3608773)] 14 Sep 2023



# Other ML papers


# Random comments (will classify)

I want to take the opportunity to share which papers I kept, and my thoughts on reading paper.

I have not written any real papers.

Authors want to get their work published.

You need to understand the motivation of the author

Academia is not that efficient. My estimate is that the industrial labs has more impact per unit spend. Of course, there are labs that create more impact than the others.

There is a parable where you visit a sensei, and your cup is full you cannot pour any more you need to empty your cup

Cite eugeneyan on something about learning.

Learning by doing?

You will be talking to chatbots and asking you what is new. It is like you are the prof and you have a graduate student that understands you well. People should publish their conversations with the paper.

Examples are important. I read the examples first before the abstract. Examples communicate taste the best.

What exactly the model did not say. Even papers contributing to the frontier of open source do not declare everything

Capture the author's intuitions.

Writing papers is performative. Reading papers is performative.

This is list not meant to be a list of papers which are better. It is just printouts that I decide to keep for now.

Good to review old papers to understand what people are thinking at that time.

I can't read without a pen.

## Footnotes

