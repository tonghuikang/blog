---
layout: post
title: Reading papers
---
I am moving from one place to another. Over my stay in the United States, I have printed a plenty of papers. These are the papers I have kept as I moved[^disclaimer].

[^disclaimer]: This just means that I previously printed the papers, and I did not discard the papers as I moved. There are very impactful papers in the field that I did not print. There are also papers in the list which I had not really read.


## Large Language Model papers


Training guides
- How to Scale Your Model [[link](https://jax-ml.github.io/scaling-book/)] Google DeepMind, 2025


GRPO and variants

- Group Sequence Policy Optimization
- Understanding R1-Zero-Like Training: A Critical Perspective [[link](https://arxiv.org/abs/2503.20783)] Sea AI Lab, 26 Mar 2025
- Why RLHF (and Other RL-Like Methods) Don't Bring True RL to LLMs [[link](https://www.linkedin.com/pulse/why-rlhf-other-rl-like-methods-dont-bring-true-rl-llmsand-atlas-wang-s1efc/)] Atlas Wang, 2025
- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models [[link](https://arxiv.org/abs/2402.03300)] DeepSeek, 5 Feb 2024
- KTO: Model Alignment as Prospect Theoretic Optimization [[link](https://arxiv.org/abs/2402.01306)] Cohere, 2 Feb 2024
- ORPO: Monolithic Preference Optimization without Reference Model [[link](https://arxiv.org/abs/2403.07691)] KAIST, 12 Mar 2024
- Direct Preference Optimization: Your Language Model is Secretly a Reward Model [[link](https://arxiv.org/abs/2305.18290)] Stanford, 29 May 2023
- A General Theoretical Paradigm to Understand Learning from Human Preferences [[link](https://arxiv.org/abs/2310.12036)] DeepMind, 18 Oct 2023


Technical reports

- Mixtral of Experts [[link](https://arxiv.org/abs/2401.04088)] Mistral AI, 8 Jan 2024
- Kimi k1.5: Scaling Reinforcement Learning with LLMs [[link](https://huggingface.co/papers/2501.12599)] Moonshot AI, 20 Jan 2025
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning [[link](https://arxiv.org/abs/2501.12948)] DeepSeek, 22 Jan 2025
- GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models [[link](https://arxiv.org/abs/2508.06471)] Zhipu AI, 20 Jun 2025
- The Llama 3 Herd of Models [[link](https://arxiv.org/abs/2407.21783)] Meta, 31 Jul 2024
- LLaMA: Open and Efficient Foundation Language Models [[link](https://arxiv.org/abs/2302.13971)] Meta, 27 Feb 2023


Efficiency efforts

- LoRA: Low-Rank Adaptation of Large Language Models [[link](https://arxiv.org/abs/2106.09685)] Microsoft, 17 Jun 2021
- Hyena Hierarchy: Towards Larger Convolutional Language Models [[link](https://arxiv.org/abs/2302.10866)] Stanford, 21 Feb 2023


Prompting

- Let's Verify Step by Step [[link](https://arxiv.org/abs/2305.20050)] OpenAI, 31 May 2023
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[link](https://arxiv.org/abs/2201.11903)] Google, 28 Jan 2022
- In-Context Learning for Extreme Multi-Label Classification [[link](https://arxiv.org/abs/2401.12178)] Ghent University, 22 Jan 2024


Early alignment efforts

- Training Language Models to Follow Instructions with Human Feedback [[link](https://arxiv.org/abs/2203.02155)] OpenAI, 4 Mar 2022
- Reinforced Self-Training (ReST) for Language Modeling [[link](https://arxiv.org/abs/2308.08998)] DeepMind, 17 Aug 2023
- RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback [[link](https://arxiv.org/abs/2309.00267)] Google, 1 Sep 2023
- Constitutional AI: Harmlessness from AI Feedback [[link](https://arxiv.org/abs/2212.08073)] Anthropic, 15 Dec 2022


## Recommendation Systems papers



Two tower model

- Self-supervised Learning for Large-scale Item Recommendations [[link](https://arxiv.org/abs/2007.12865)] Google, 25 Jul 2020
- Cross-Batch Negative Sampling for Training Two-Tower Recommenders [[link](https://arxiv.org/abs/2110.15154)] Huawei, 28 Oct 2021
- Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations [[link](https://dl.acm.org/doi/10.1145/3366424.3386195)] Google, 20 Apr 2020
- Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations [[link](https://dl.acm.org/doi/abs/10.1145/3298689.3346996)] Google, 10 Sep 2019
- Deep Neural Networks for YouTube Recommendations [[link](https://dl.acm.org/doi/10.1145/2959100.2959190)] Google, 15 Sep 2016


User interest exploration

- Values of User Exploration in Recommender Systems [[link](https://dl.acm.org/doi/10.1145/3460231.3474236)] Google, 13 Sep 2021


Item exploration

- Nonlinear Bandits Exploration for Recommendations [[link](https://dl.acm.org/doi/10.1145/3604915.3610245)] Google, 14 Sep 2023
- Online Matching: A Real-time Bandit System for Large-scale Recommendations [[link](https://arxiv.org/abs/2307.15893)] Google, 29 Jul 2023
- Long-Term Value of Exploration: Measurements, Findings and Algorithms [[link](https://arxiv.org/abs/2305.07764)] Google, 12 May 2023
- Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation [[link](https://arxiv.org/abs/2306.01720)] Google, 2 Jun 2023


Trainable embeddings

- Monolith: Real Time Recommendation System With Collisionless Embedding Table [[link](https://arxiv.org/abs/2209.07663)] ByteDance, 16 Sep 2022
- Efficient Data Representation Learning in Google-scale Systems [[link](https://dl.acm.org/doi/10.1145/3604915.3608882)] Google, 14 Sep 2023



Pretrained embeddings

- Cross-lingual Language Model Pretraining [[link](https://arxiv.org/abs/1901.07291)] Facebook AI, 22 Jan 2019
- Text Embeddings by Weakly-Supervised Contrastive Pre-training [[link](https://arxiv.org/abs/2212.03533)] Microsoft, 7 Dec 2022



Sequence feature modelling

- TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest [[link](https://arxiv.org/abs/2306.00248)] Pinterest, 1 Jun 2023
- Behavior Sequence Transformer for E-commerce Recommendation in Alibaba [[link](https://arxiv.org/abs/1905.06874)] Alibaba, 16 May 2019
- Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction [[link](https://arxiv.org/abs/2006.05639)] Alibaba, 10 Jun 2020
- DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems [[link](https://arxiv.org/abs/2008.13535)] Google, 31 Aug 2020
- Deep Interest Network for Click-Through Rate Prediction [[link](https://arxiv.org/abs/1706.06978)] Alibaba, 21 Jun 2017



Calibration

- On Calibration of Modern Neural Networks [[link](https://arxiv.org/abs/1706.04599)] Cornell University, 14 Jun 2017
- Why Model Calibration Matters and How to Achieve It [[link](https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html)] Google, Apr 2021
- Multi-task Learning and Calibration for Utility-based Home Feed Ranking [[link](https://medium.com/pinterest-engineering/multi-task-learning-and-calibration-for-utility-based-home-feed-ranking-64087a7bcbad)] Pinterest, 14 Sep 2020
- The Foundations of Cost-Sensitive Learning [[link](https://dl.acm.org/doi/10.5555/1642194.1642224)] UCSD, 4 Aug 2001
- Predicting Good Probabilities with Supervised Learning [[link](https://dl.acm.org/doi/10.1145/1102351.1102430)] Cornell, 7 Aug 2005



Training multi-task models

- Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts [[link](https://dl.acm.org/doi/10.1145/3219819.3220007)] Google, 13 Jun 2018
- Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations [[link](https://dl.acm.org/doi/10.1145/3383313.3412236)] Tencent, 22 Sep 2020
- Recommending What Video to Watch Next: A Multitask Ranking System [[link](https://dl.acm.org/doi/10.1145/3298689.3346997)] Google, 10 Sep 2019



Value modelling

- What We Know About Using Non-Engagement Signals in Content Ranking [[link](https://arxiv.org/abs/2402.06831)] Integrity Institute, 9 Feb 2024
- Multi-Objective Recommendation via Multivariate Policy Learning [[link](https://arxiv.org/abs/2405.02141)] Spotify, 3 May 2024
- Feedback Shaping: A Modeling Approach to Nurture Content Creation [[link](https://arxiv.org/abs/2106.11312)] LinkedIn, 21 Jun 2021



Recommendation as sequence prediction

- Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations [[link](https://arxiv.org/abs/2402.17152)] Meta, 26 Feb 2024
- Effective and Efficient Training for Sequential Recommendation using Recency Sampling [[link](https://arxiv.org/abs/2207.02643)] University of Glasgow, 6 Jul 2022
- BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer [[link](https://arxiv.org/abs/1904.06690)] Alibaba, 15 Apr 2019
- Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders [[link](https://arxiv.org/abs/2308.12256)] Google, 23 Aug 2023



Deep retrieval

- Deep Retrieval: Learning A Retrievable Structure for Large-Scale Recommendations [[link](https://arxiv.org/abs/2007.07203)] ByteDance, 14 Jul 2020



Miscellaneous

- Improving Training Stability for Multitask Ranking Models in Recommender Systems [[link](https://arxiv.org/abs/2302.09178)] Google, 18 Feb 2023
- Trustworthy Online Marketplace Experimentation with Budget-split Design [[link](https://arxiv.org/abs/2012.08724)] LinkedIn, 16 Dec 2020
- Why do tree-based models still outperform deep learning on tabular data? [[link](https://arxiv.org/abs/2207.08815)] 18 Jul 2022
- Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models [[link](https://arxiv.org/abs/2209.06053)] Alibaba, 13 Sep 2022
- Fairness in Recommendation Ranking through Pairwise Comparisons [[link](https://arxiv.org/abs/1903.00780)] Google, 2 Mar 2019
- Practical Lessons from Predicting Clicks on Ads at Facebook [[link](https://dl.acm.org/doi/10.1145/2648584.2648589)] Facebook, 24 Aug 2014
- Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [[link](https://arxiv.org/abs/1603.09320)] Russian Academy of Sciences, 30 Mar 2016
- Deep Retrieval: Learning A Retrievable Structure for Large-Scale Recommendations [[link](https://arxiv.org/abs/2007.07203)] ByteDance, 14 Jul 2020
- Full Index Deep Retrieval: End-to-End User and Item Structures for Cold-start and Long-tail Item Recommendation [[link](https://dl.acm.org/doi/10.1145/3604915.3608773)] 14 Sep 2023



## Other ML resources

Reinforcement learning

- Reinforcement Learning: An Introduction [[link](http://incompleteideas.net/book/the-book-2nd.html)] Sutton & Barto, 2018


Image models

- High-Resolution Image Synthesis with Latent Diffusion Models [[link](https://arxiv.org/abs/2112.10752)] LMU Munich, 20 Dec 2021
- An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [[link](https://arxiv.org/abs/2010.11929)] Google, 22 Oct 2020


Attention mechanism

- Attention Is All You Need [[link](https://arxiv.org/abs/1706.03762)] Google, 12 Jun 2017
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[link](https://arxiv.org/abs/1810.04805)] Google, 11 Oct 2018
- Long Short-Term Memory-Networks for Machine Reading [[link](https://arxiv.org/abs/1601.06733)] University of Edinburgh, 25 Jan 2016
- Neural Machine Translation by Jointly Learning to Align and Translate [[link](https://arxiv.org/abs/1409.0473)] Mila, 1 Sep 2014


# Random comments (will classify)

I want to take the opportunity to share which papers I kept, and my thoughts on reading paper.

I have not written any real papers.

Authors want to get their work published.

You need to understand the motivation of the author

Academia is not that efficient. My estimate is that the industrial labs has more impact per unit spend. Of course, there are labs that create more impact than the others.

There is a parable where you visit a sensei, and your cup is full you cannot pour any more you need to empty your cup

Cite eugeneyan on something about learning.

Learning by doing?

You will be talking to chatbots and asking you what is new. It is like you are the prof and you have a graduate student that understands you well. People should publish their conversations with the paper.

Examples are important. I read the examples first before the abstract. Examples communicate taste the best.

What exactly the model did not say. Even papers contributing to the frontier of open source do not declare everything

Capture the author's intuitions.

Writing papers is performative. Reading papers is performative.

This is list not meant to be a list of papers which are better. It is just printouts that I decide to keep for now.

Good to review old papers to understand what people are thinking at that time.

I can't read without a pen.

## Footnotes

